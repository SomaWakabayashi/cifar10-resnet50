{"cells":[{"cell_type":"code","execution_count":2,"id":"2cf1d9c5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27048,"status":"ok","timestamp":1766632794820,"user":{"displayName":"Soma Wakabayashi","userId":"02898637339021765080"},"user_tz":-540},"id":"2cf1d9c5","outputId":"0a761b60-9e79-44b1-d592-27a4f555cd4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Colabサーバー側でGoogle Driveをマウント\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# 2. プロジェクトの場所へ移動 (ここが抜けていました！)\n","# ※パスは前回作成した場所に合わせています\n","%cd \"/content/drive/MyDrive/Spark+/Recruitment_Tasks/cifar10-resnet50\"\n","\n","# 3. ファイルがあるか確認 (train.py が表示されればOK)\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hhnk45Z6JI6V","executionInfo":{"status":"ok","timestamp":1766632797736,"user_tz":-540,"elapsed":2912,"user":{"displayName":"Soma Wakabayashi","userId":"02898637339021765080"}},"outputId":"47286f8e-8182-4aba-927c-1fcf3a9020cf"},"id":"Hhnk45Z6JI6V","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Spark+/Recruitment_Tasks/cifar10-resnet50\n","analysis.ipynb\t\t      requirements.txt\n","comparison_val_acc.png\t      runner.ipynb\n","comparison_val_loss.png       runs\n","configs\t\t\t      train_cifar10_resnet50_advanced.py\n","data\t\t\t      train_cifar.py\n","evolution_comparison.png      train_fast.py\n","final_experiment_summary.csv  train.py\n","notebooks\t\t      train_resume.py\n","predict.py\t\t      utils\n","__pycache__\t\t      Validation_comparison.png\n","README.md\n"]}]},{"cell_type":"code","execution_count":null,"id":"AdBXv_3pdG-h","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1795830,"status":"ok","timestamp":1766577947547,"user":{"displayName":"Soma Wakabayashi","userId":"02898637339021765080"},"user_tz":-540},"id":"AdBXv_3pdG-h","outputId":"09b54a86-fee1-4679-aeb4-626434318cf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Seed fixed to 42\n","Epoch [1/40] Val Acc: 21.17%\n","Epoch [2/40] Val Acc: 33.86%\n","Epoch [3/40] Val Acc: 38.23%\n","Epoch [4/40] Val Acc: 44.88%\n","Epoch [5/40] Val Acc: 44.56%\n","Epoch [6/40] Val Acc: 47.77%\n","Epoch [7/40] Val Acc: 48.52%\n","Epoch [8/40] Val Acc: 57.02%\n","Epoch [9/40] Val Acc: 58.06%\n","Epoch [10/40] Val Acc: 59.00%\n","Epoch [11/40] Val Acc: 56.24%\n","Epoch [12/40] Val Acc: 63.15%\n","Epoch [13/40] Val Acc: 65.26%\n","Epoch [14/40] Val Acc: 63.98%\n","Epoch [15/40] Val Acc: 67.44%\n","Epoch [16/40] Val Acc: 66.78%\n","Epoch [17/40] Val Acc: 68.08%\n","Epoch [18/40] Val Acc: 68.27%\n","Epoch [19/40] Val Acc: 70.48%\n","Epoch [20/40] Val Acc: 67.27%\n","Epoch [21/40] Val Acc: 72.72%\n","Epoch [22/40] Val Acc: 72.08%\n","Epoch [23/40] Val Acc: 72.07%\n","Epoch [24/40] Val Acc: 75.16%\n","Epoch [25/40] Val Acc: 75.01%\n","Epoch [26/40] Val Acc: 74.96%\n","Epoch [27/40] Val Acc: 75.12%\n","Epoch [28/40] Val Acc: 75.45%\n","Epoch [29/40] Val Acc: 78.03%\n","Epoch [30/40] Val Acc: 79.15%\n","Epoch [31/40] Val Acc: 81.54%\n","Epoch [32/40] Val Acc: 81.08%\n","Epoch [33/40] Val Acc: 82.00%\n","Epoch [34/40] Val Acc: 83.55%\n","Epoch [35/40] Val Acc: 84.17%\n","Epoch [36/40] Val Acc: 85.20%\n","Epoch [37/40] Val Acc: 85.11%\n","Epoch [38/40] Val Acc: 85.78%\n","Epoch [39/40] Val Acc: 86.15%\n","Epoch [40/40] Val Acc: 86.32%\n","✅ Saved to runs/aug_reg\n"]}],"source":["!python train.py --exp_name \"aug_reg\" --augment standard --label_smoothing 0.1 --weight_decay 1e-3 --epochs 40"]},{"cell_type":"code","source":["# 実験名: cifar_optimized\n","# 最適化モデル + Label Smoothing(0.1) + Weight Decay(1e-3)\n","!python train_cifar.py --exp_name \"cifar_optimized\" --augment standard --epochs 40"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaNq8y6eaEnR","executionInfo":{"status":"ok","timestamp":1766583381533,"user_tz":-540,"elapsed":2570928,"user":{"displayName":"Soma Wakabayashi","userId":"02898637339021765080"}},"outputId":"973b725b-afc8-4560-8d56-9ceb41a144cb"},"id":"eaNq8y6eaEnR","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Seed fixed to 42\n","/content/drive/MyDrive/Spark+/Recruitment_Tasks/cifar10-resnet50/train_cifar.py:185: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n","Training:   0% 0/391 [00:00<?, ?it/s]/content/drive/MyDrive/Spark+/Recruitment_Tasks/cifar10-resnet50/train_cifar.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  return torch._C._get_cublas_allow_tf32()\n","W1224 12:55:02.328000 1633 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","Training:   0% 1/391 [01:14<8:02:36, 74.25s/it]/content/drive/MyDrive/Spark+/Recruitment_Tasks/cifar10-resnet50/train_cifar.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Evaluating:   0% 0/79 [00:00<?, ?it/s]/content/drive/MyDrive/Spark+/Recruitment_Tasks/cifar10-resnet50/train_cifar.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Evaluating:   1% 1/79 [00:22<29:33, 22.73s/it]/content/drive/MyDrive/Spark+/Recruitment_Tasks/cifar10-resnet50/train_cifar.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Epoch [1/40] Val Acc: 18.88%\n","Training:   0% 0/391 [00:00<?, ?it/s]/content/drive/MyDrive/Spark+/Recruitment_Tasks/cifar10-resnet50/train_cifar.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Epoch [2/40] Val Acc: 27.54%\n","Epoch [3/40] Val Acc: 38.49%\n","Epoch [4/40] Val Acc: 44.26%\n","Epoch [5/40] Val Acc: 51.47%\n","Epoch [6/40] Val Acc: 57.05%\n","Epoch [7/40] Val Acc: 61.55%\n","Epoch [8/40] Val Acc: 66.41%\n","Epoch [9/40] Val Acc: 60.61%\n","Epoch [10/40] Val Acc: 63.64%\n","Epoch [11/40] Val Acc: 58.67%\n","Epoch [12/40] Val Acc: 67.75%\n","Epoch [13/40] Val Acc: 60.37%\n","Epoch [14/40] Val Acc: 70.39%\n","Epoch [15/40] Val Acc: 69.63%\n","Epoch [16/40] Val Acc: 76.45%\n","Epoch [17/40] Val Acc: 74.70%\n","Epoch [18/40] Val Acc: 67.91%\n","Epoch [19/40] Val Acc: 73.72%\n","Epoch [20/40] Val Acc: 74.67%\n","Epoch [21/40] Val Acc: 78.04%\n","Epoch [22/40] Val Acc: 75.57%\n","Epoch [23/40] Val Acc: 79.36%\n","Epoch [24/40] Val Acc: 81.71%\n","Epoch [25/40] Val Acc: 77.15%\n","Epoch [26/40] Val Acc: 78.21%\n","Epoch [27/40] Val Acc: 84.18%\n","Epoch [28/40] Val Acc: 85.14%\n","Epoch [29/40] Val Acc: 82.29%\n","Epoch [30/40] Val Acc: 87.75%\n","Epoch [31/40] Val Acc: 89.28%\n","Epoch [32/40] Val Acc: 88.33%\n","Epoch [33/40] Val Acc: 88.79%\n","Epoch [34/40] Val Acc: 90.61%\n","Epoch [35/40] Val Acc: 91.19%\n","Epoch [36/40] Val Acc: 91.70%\n","Epoch [37/40] Val Acc: 92.33%\n","Epoch [38/40] Val Acc: 92.77%\n","Epoch [39/40] Val Acc: 92.83%\n","Epoch [40/40] Val Acc: 92.80%\n","/content/drive/MyDrive/Spark+/Recruitment_Tasks/cifar10-resnet50/train_cifar.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","✅ Training Complete. Saved to runs/cifar_optimized\n"]}]},{"cell_type":"code","source":["# 実行コマンド\n","!python train_cifar10_resnet50_advanced.py --exp_name \"resnet50_cifar10_all_aug_reg\" --epochs 40"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZupVhgV0OMA","executionInfo":{"status":"ok","timestamp":1766590537856,"user_tz":-540,"elapsed":3077052,"user":{"displayName":"Soma Wakabayashi","userId":"02898637339021765080"}},"outputId":"b25349c6-5e11-40c1-8fdf-a4f52904887e"},"id":"1ZupVhgV0OMA","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Seed fixed to 42\n","Training:   0% 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  return torch._C._get_cublas_allow_tf32()\n","W1224 14:45:11.727000 590 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","Epoch [1/40] Val Acc: 13.30%\n","Epoch [2/40] Val Acc: 15.62%\n","Epoch [3/40] Val Acc: 26.24%\n","Epoch [4/40] Val Acc: 36.21%\n","Epoch [5/40] Val Acc: 42.74%\n","Epoch [6/40] Val Acc: 44.88%\n","Epoch [7/40] Val Acc: 49.30%\n","Epoch [8/40] Val Acc: 54.94%\n","Epoch [9/40] Val Acc: 51.41%\n","Epoch [10/40] Val Acc: 54.75%\n","Epoch [11/40] Val Acc: 49.32%\n","Epoch [12/40] Val Acc: 64.39%\n","Epoch [13/40] Val Acc: 55.45%\n","Epoch [14/40] Val Acc: 40.88%\n","Epoch [15/40] Val Acc: 60.80%\n","Epoch [16/40] Val Acc: 55.12%\n","Epoch [17/40] Val Acc: 59.65%\n","Epoch [18/40] Val Acc: 55.88%\n","Epoch [19/40] Val Acc: 64.29%\n","Epoch [20/40] Val Acc: 69.80%\n","Epoch [21/40] Val Acc: 70.59%\n","Epoch [22/40] Val Acc: 66.96%\n","Epoch [23/40] Val Acc: 71.02%\n","Epoch [24/40] Val Acc: 75.32%\n","Epoch [25/40] Val Acc: 76.74%\n","Epoch [26/40] Val Acc: 78.61%\n","Epoch [27/40] Val Acc: 79.45%\n","Epoch [28/40] Val Acc: 79.25%\n","Epoch [29/40] Val Acc: 79.52%\n","Epoch [30/40] Val Acc: 81.16%\n","Epoch [31/40] Val Acc: 83.58%\n","Epoch [32/40] Val Acc: 85.62%\n","Epoch [33/40] Val Acc: 85.51%\n","Epoch [34/40] Val Acc: 86.09%\n","Epoch [35/40] Val Acc: 87.15%\n","Epoch [36/40] Val Acc: 88.82%\n","Epoch [37/40] Val Acc: 89.05%\n","Epoch [38/40] Val Acc: 89.01%\n","Epoch [39/40] Val Acc: 89.85%\n","Epoch [40/40] Val Acc: 90.03%\n","✅ Experiment 'resnet50_cifar10_all_aug_reg' Complete.\n"]}]},{"cell_type":"code","source":["!python train_resume.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uns6s5QzGkp-","outputId":"dcbdab00-e006-4531-cbbe-9060d61fc5fc"},"id":"uns6s5QzGkp-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Seed fixed to 42\n",">>> Loading checkpoint: runs/resnet50_cifar10_all_aug_reg/model.pth\n",">>> Resuming from epoch 41. Training for 60 more epochs...\n","Epoch [41/100] Val Acc: 74.53%\n","Epoch [42/100] Val Acc: 81.28%\n","Epoch [43/100] Val Acc: 81.42%\n","Epoch [44/100] Val Acc: 79.44%\n","Epoch [45/100] Val Acc: 81.99%\n","Epoch [46/100] Val Acc: 80.94%\n","Epoch [47/100] Val Acc: 82.96%\n","Epoch [48/100] Val Acc: 82.25%\n","Epoch [49/100] Val Acc: 80.73%\n","Epoch [50/100] Val Acc: 83.01%\n","Epoch [51/100] Val Acc: 85.20%\n","Epoch [52/100] Val Acc: 83.51%\n","Epoch [53/100] Val Acc: 82.82%\n","Epoch [54/100] Val Acc: 84.37%\n","Epoch [55/100] Val Acc: 84.47%\n","Epoch [56/100] Val Acc: 85.60%\n","Epoch [57/100] Val Acc: 86.73%\n","Epoch [58/100] Val Acc: 84.24%\n","Epoch [59/100] Val Acc: 83.56%\n","Epoch [60/100] Val Acc: 83.49%\n","Epoch [61/100] Val Acc: 86.97%\n","Epoch [62/100] Val Acc: 85.93%\n","Epoch [63/100] Val Acc: 85.61%\n","Training:  94% 366/391 [01:02<00:03,  6.82it/s]"]}]},{"cell_type":"code","source":["!python train_resume.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jEOBxHzuXa-a","executionInfo":{"status":"ok","timestamp":1766599749889,"user_tz":-540,"elapsed":3089592,"user":{"displayName":"Soma Wakabayashi","userId":"02898637339021765080"}},"outputId":"e997ede7-4c78-4965-c49d-0710c6811ef2"},"id":"jEOBxHzuXa-a","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Seed fixed to 42\n",">>> Current progress: 60 epochs. Goal: 100.\n",">>> Remaining: 40 epochs.\n","/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\n","Training:   0% 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  return torch._C._get_cublas_allow_tf32()\n","W1224 17:18:34.953000 586 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","Epoch [61/100] Val Acc: 83.20% (LR: 0.014540)\n","Epoch [62/100] Val Acc: 86.59% (LR: 0.014067)\n","Epoch [63/100] Val Acc: 85.79% (LR: 0.013584)\n","Epoch [64/100] Val Acc: 84.58% (LR: 0.013090)\n","Epoch [65/100] Val Acc: 87.68% (LR: 0.012588)\n","Epoch [66/100] Val Acc: 85.20% (LR: 0.012079)\n","Epoch [67/100] Val Acc: 88.61% (LR: 0.011564)\n","Epoch [68/100] Val Acc: 87.30% (LR: 0.011045)\n","Epoch [69/100] Val Acc: 85.13% (LR: 0.010523)\n","Epoch [70/100] Val Acc: 88.02% (LR: 0.010000)\n","Epoch [71/100] Val Acc: 88.77% (LR: 0.009477)\n","Epoch [72/100] Val Acc: 88.48% (LR: 0.008955)\n","Epoch [73/100] Val Acc: 88.34% (LR: 0.008436)\n","Epoch [74/100] Val Acc: 88.27% (LR: 0.007921)\n","Epoch [75/100] Val Acc: 89.90% (LR: 0.007412)\n","Epoch [76/100] Val Acc: 90.24% (LR: 0.006910)\n","Epoch [77/100] Val Acc: 90.73% (LR: 0.006416)\n","Epoch [78/100] Val Acc: 91.27% (LR: 0.005933)\n","Epoch [79/100] Val Acc: 90.89% (LR: 0.005460)\n","Epoch [80/100] Val Acc: 91.38% (LR: 0.005000)\n","Epoch [81/100] Val Acc: 91.97% (LR: 0.004554)\n","Epoch [82/100] Val Acc: 91.31% (LR: 0.004122)\n","Epoch [83/100] Val Acc: 92.57% (LR: 0.003707)\n","Epoch [84/100] Val Acc: 92.39% (LR: 0.003309)\n","Epoch [85/100] Val Acc: 92.02% (LR: 0.002929)\n","Epoch [86/100] Val Acc: 92.89% (LR: 0.002569)\n","Epoch [87/100] Val Acc: 92.98% (LR: 0.002229)\n","Epoch [88/100] Val Acc: 93.26% (LR: 0.001910)\n","Epoch [89/100] Val Acc: 93.09% (LR: 0.001613)\n","Epoch [90/100] Val Acc: 93.99% (LR: 0.001340)\n","Epoch [91/100] Val Acc: 94.26% (LR: 0.001090)\n","Epoch [92/100] Val Acc: 94.21% (LR: 0.000865)\n","Epoch [93/100] Val Acc: 94.25% (LR: 0.000664)\n","Epoch [94/100] Val Acc: 94.42% (LR: 0.000489)\n","Epoch [95/100] Val Acc: 94.66% (LR: 0.000341)\n","Epoch [96/100] Val Acc: 94.70% (LR: 0.000219)\n","Epoch [97/100] Val Acc: 94.78% (LR: 0.000123)\n","Epoch [98/100] Val Acc: 94.65% (LR: 0.000055)\n","Epoch [99/100] Val Acc: 94.75% (LR: 0.000014)\n","Epoch [100/100] Val Acc: 94.86% (LR: 0.000000)\n","✅ Successfully reached 100 epochs.\n"]}]},{"cell_type":"code","source":["# 100エポック一気通貫。これがプロジェクトの「正典（Canonical）」データになります。\n","!python train_cifar10_resnet50_advanced.py --exp_name \"resnet50_cifar10_pure_100e\" --epochs 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ONcMRcqfhJIC","executionInfo":{"status":"ok","timestamp":1766640036881,"user_tz":-540,"elapsed":7239143,"user":{"displayName":"Soma Wakabayashi","userId":"02898637339021765080"}},"outputId":"5cf181b5-55b4-42f2-a808-dd036304b38e"},"id":"ONcMRcqfhJIC","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Seed fixed to 42\n","Training:   0% 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  return torch._C._get_cublas_allow_tf32()\n","W1225 03:21:04.809000 1086 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n","Epoch [1/100] Val Acc: 13.30%\n","Epoch [2/100] Val Acc: 16.19%\n","Epoch [3/100] Val Acc: 26.72%\n","Epoch [4/100] Val Acc: 36.24%\n","Epoch [5/100] Val Acc: 38.63%\n","Epoch [6/100] Val Acc: 40.46%\n","Epoch [7/100] Val Acc: 50.43%\n","Epoch [8/100] Val Acc: 52.56%\n","Epoch [9/100] Val Acc: 50.08%\n","Epoch [10/100] Val Acc: 55.50%\n","Epoch [11/100] Val Acc: 51.85%\n","Epoch [12/100] Val Acc: 60.05%\n","Epoch [13/100] Val Acc: 64.34%\n","Epoch [14/100] Val Acc: 55.09%\n","Epoch [15/100] Val Acc: 61.50%\n","Epoch [16/100] Val Acc: 57.97%\n","Epoch [17/100] Val Acc: 65.37%\n","Epoch [18/100] Val Acc: 66.09%\n","Epoch [19/100] Val Acc: 62.01%\n","Epoch [20/100] Val Acc: 66.26%\n","Epoch [21/100] Val Acc: 66.28%\n","Epoch [22/100] Val Acc: 56.00%\n","Epoch [23/100] Val Acc: 65.06%\n","Epoch [24/100] Val Acc: 66.74%\n","Epoch [25/100] Val Acc: 69.22%\n","Epoch [26/100] Val Acc: 69.21%\n","Epoch [27/100] Val Acc: 69.91%\n","Epoch [28/100] Val Acc: 67.52%\n","Epoch [29/100] Val Acc: 67.87%\n","Epoch [30/100] Val Acc: 71.45%\n","Epoch [31/100] Val Acc: 69.40%\n","Epoch [32/100] Val Acc: 65.56%\n","Epoch [33/100] Val Acc: 68.21%\n","Epoch [34/100] Val Acc: 72.27%\n","Epoch [35/100] Val Acc: 68.13%\n","Epoch [36/100] Val Acc: 69.82%\n","Epoch [37/100] Val Acc: 72.74%\n","Epoch [38/100] Val Acc: 70.62%\n","Epoch [39/100] Val Acc: 65.39%\n","Epoch [40/100] Val Acc: 74.30%\n","Epoch [41/100] Val Acc: 73.99%\n","Epoch [42/100] Val Acc: 70.09%\n","Epoch [43/100] Val Acc: 72.07%\n","Epoch [44/100] Val Acc: 75.22%\n","Epoch [45/100] Val Acc: 74.46%\n","Epoch [46/100] Val Acc: 74.47%\n","Epoch [47/100] Val Acc: 75.41%\n","Epoch [48/100] Val Acc: 77.22%\n","Epoch [49/100] Val Acc: 78.33%\n","Epoch [50/100] Val Acc: 73.81%\n","Epoch [51/100] Val Acc: 76.51%\n","Epoch [52/100] Val Acc: 77.16%\n","Epoch [53/100] Val Acc: 72.37%\n","Epoch [54/100] Val Acc: 78.65%\n","Epoch [55/100] Val Acc: 75.62%\n","Epoch [56/100] Val Acc: 78.90%\n","Epoch [57/100] Val Acc: 80.47%\n","Epoch [58/100] Val Acc: 75.38%\n","Epoch [59/100] Val Acc: 80.23%\n","Epoch [60/100] Val Acc: 81.35%\n","Epoch [61/100] Val Acc: 76.93%\n","Epoch [62/100] Val Acc: 79.43%\n","Epoch [63/100] Val Acc: 80.76%\n","Epoch [64/100] Val Acc: 77.43%\n","Epoch [65/100] Val Acc: 83.89%\n","Epoch [66/100] Val Acc: 79.83%\n","Epoch [67/100] Val Acc: 81.53%\n","Epoch [68/100] Val Acc: 83.74%\n","Epoch [69/100] Val Acc: 82.15%\n","Epoch [70/100] Val Acc: 82.09%\n","Epoch [71/100] Val Acc: 86.31%\n","Epoch [72/100] Val Acc: 84.96%\n","Epoch [73/100] Val Acc: 85.50%\n","Epoch [74/100] Val Acc: 86.96%\n","Epoch [75/100] Val Acc: 86.61%\n","Epoch [76/100] Val Acc: 86.98%\n","Epoch [77/100] Val Acc: 85.39%\n","Epoch [78/100] Val Acc: 87.11%\n","Epoch [79/100] Val Acc: 90.12%\n","Epoch [80/100] Val Acc: 88.40%\n","Epoch [81/100] Val Acc: 89.86%\n","Epoch [82/100] Val Acc: 89.45%\n","Epoch [83/100] Val Acc: 89.82%\n","Epoch [84/100] Val Acc: 90.40%\n","Epoch [85/100] Val Acc: 91.87%\n","Epoch [86/100] Val Acc: 91.22%\n","Epoch [87/100] Val Acc: 92.22%\n","Epoch [88/100] Val Acc: 92.57%\n","Epoch [89/100] Val Acc: 92.25%\n","Epoch [90/100] Val Acc: 92.37%\n","Epoch [91/100] Val Acc: 92.78%\n","Epoch [92/100] Val Acc: 93.53%\n","Epoch [93/100] Val Acc: 93.38%\n","Epoch [94/100] Val Acc: 93.80%\n","Epoch [95/100] Val Acc: 93.55%\n","Epoch [96/100] Val Acc: 94.15%\n","Epoch [97/100] Val Acc: 94.06%\n","Epoch [98/100] Val Acc: 94.17%\n","Epoch [99/100] Val Acc: 94.13%\n","Epoch [100/100] Val Acc: 94.39%\n","✅ Experiment 'resnet50_cifar10_pure_100e' Complete.\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"}},"nbformat":4,"nbformat_minor":5}